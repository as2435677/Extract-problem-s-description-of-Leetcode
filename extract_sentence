#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 10 11:49:18 2020

@author: ken
"""

from stanfordcorenlp import StanfordCoreNLP
import os

#Tool Announcement
nlp = StanfordCoreNLP(r'/home/ken/stanford-corenlp-4.1.0')

#input and output keywords sets
input_relateword_sets = [
    "input",
    "given",
    "give",
    "provide",
    "receive",
    "take"
    ]

output_relateword_sets = [
    "output",
    "return",
    "obtain"
    ]

sentences = []

dir_path = "/home/ken/leetcode_web_crawler/descriptions/"
filenames = os.listdir(dir_path)
fo = open("sentences.txt", "w+")

def recover_sentence(tokens):
    sen = ''
    #if flag = 1, need to reduce one space
    flag = 0
    for i in range(len(tokens)):
        if tokens[i] == 'ROOT':
            continue
        elif tokens[i] == '-':
            sen = sen + tokens[i]
            flag = 1
        elif (i != len(tokens)-1) and ("'" in tokens[i+1]):
            sen = sen + ' ' + tokens[i]
            flag = 1
        else:
            if flag == 1:
                sen = sen + tokens[i]
                flag = 0
                continue
            sen = sen + ' ' + tokens[i]
    return sen

def remove_useless_tokens(tokens):
    new_tokens = []
    for token in tokens:
        if '<' not in token:
            new_tokens.append(token)
    return new_tokens

def check_ioput_relateword(tokens):
    input_flag = 0
    output_flag = 0
    for data in input_relateword_sets:
        if data in tokens:
            input_flag = 1
            break
    for data in output_relateword_sets:
        if data in tokens:
            output_flag = 1
            break
    return input_flag, output_flag

i = 0
flag = 0
total_files = len(filenames)

while i < total_files:
    if flag == 0:
        target_file = dir_path + filenames[i]
        f_sen = open(target_file, "r+")
        flag = 1
    line = f_sen.readline()
    if line == "":
        i = i + 1
        flag = 0
        f_sen.close()
        continue
    tokens = nlp.word_tokenize(line)
    new_tokens = remove_useless_tokens(tokens)
    input_flag, output_flag = check_ioput_relateword(new_tokens)
    if not (input_flag | output_flag):
        continue
    sen = recover_sentence(new_tokens)
    fo.writelines("%s\n" % sen)
    #i = i + 1
    #flag = 0
    #f_sen.close()
    #break

    

fo.close()
nlp.close()